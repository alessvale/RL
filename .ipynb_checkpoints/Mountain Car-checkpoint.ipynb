{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## An implementation of the Mountain Car problem \n",
    "## via a SARSA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the environment\n",
    "\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A simple implementation of a uniform square tile coding\n",
    "\n",
    "class TileCoding:\n",
    "    \n",
    "    def __init__(self, feat_ranges, nbins, offsets):\n",
    "        \n",
    "        self.tilings = self.buildTilings(feat_ranges, nbins, offsets)\n",
    "    \n",
    "    \n",
    "    def maketiling(self, feat_range, nbins, offset):\n",
    "        \n",
    "        ##Build a 1D tiling with uniformely spaced tiles\n",
    "        tiling = np.linspace(feat_range[0], feat_range[1], nbins + 1)[1:-1] + offset\n",
    "        \n",
    "        return tiling\n",
    "\n",
    "    def makeTilings(self, feat_ranges, nbins, offset):\n",
    "        \n",
    "        ##Make a multidimensional tiling by assembling 1D tilings for each dimension\n",
    "        tilings = []\n",
    "        \n",
    "        for i in range(len(feat_ranges)):\n",
    "            feat_range = feat_ranges[i]\n",
    "            tiling = self.maketiling(feat_range, nbins, offset[i])\n",
    "            tilings.append(tiling)\n",
    "\n",
    "        return tilings\n",
    "\n",
    "\n",
    "    def buildTilings(self, feat_ranges, nbins, offsets):\n",
    "        \n",
    "        ##Make collection of offset tilengs\n",
    "        tilings = []\n",
    "\n",
    "        for offset in offsets:\n",
    "\n",
    "            tiling = self.makeTilings(feat_ranges, nbins, offset)\n",
    "            tilings.append(tiling)\n",
    "\n",
    "        return tilings\n",
    "\n",
    "    def encode(self, feature, tiling):\n",
    "        ##Check that the number of features coincide with the number of 1D tilings\n",
    "        assert len(feature) == len(tiling)\n",
    "        \n",
    "        indices = []\n",
    "        \n",
    "        ##Get indices of the active tiles in the tiling\n",
    "        for i in range(len(feature)):\n",
    "            feat_i = feature[i]\n",
    "            tiles_i = tiling[i]\n",
    "            idx = np.digitize(feat_i, tiles_i)\n",
    "            indices.append(idx)\n",
    "        \n",
    "        ##Build a vector of tiles activity\n",
    "        dims = [len(tiles) + 1 for tiles in tiling]\n",
    "        tot = int(np.prod(dims))\n",
    "        coding = np.zeros(dims)\n",
    "        coding[tuple(indices)] = 1\n",
    "\n",
    "        return coding.reshape(tot, 1)\n",
    "\n",
    "    def tile(self, feature):\n",
    "        \n",
    "        ##Concatenate the various encoded vectors\n",
    "        codings = []\n",
    "\n",
    "        for tiling in self.tilings:\n",
    "            code = self.encode(feature, tiling)\n",
    "            codings.append(code)\n",
    "\n",
    "        return np.concatenate(codings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define an agent with linear Q-function, SARSA update algorithm and experience replay \n",
    "\n",
    "BATCH_NUM = 32\n",
    "OFFSETS = [[0.0, 0.0], [0.1, 0.01], [0.3, 0.03], [-0.1, -0.01], [-1.0, -0.05], [1.0, 0.05], [-1.2, -0.06]]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "           \n",
    "        self.gamma = 1.0 ##No discount\n",
    "        self.epsilon = 0.0 ## Exploration is granted by negative reward feedback\n",
    "        self.num_actions = env.action_space.n\n",
    "        \n",
    "        ranges = self.getFeatRange(env)\n",
    "        ##Offsets for the tile coding\n",
    "        \n",
    "        \n",
    "        self.lr = 0.3/len(offsets)\n",
    "        num_bin = 8 ## Use 8 x 8 tilings\n",
    "        self.tilecode = TileCoding(ranges, num_bin, OFFSETS) ##Initialize the Tile Coding class\n",
    "        self.states = [] ## Collect states for experience replay\n",
    "        dummy = self.feat([0, 0]) ## Dummy state for debug and weights shape\n",
    "        \n",
    "        self.W = tf.Variable(np.zeros(shape = (1, dummy.shape[0] * env.action_space.n + 1)), dtype =tf.float32) ##Weights vector\n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.lr) ## Choice of optimizer\n",
    "        \n",
    "    \n",
    "    #Get the ranges of the state compontents\n",
    "    \n",
    "    def getFeatRange(self, env):\n",
    "        \n",
    "        range_low = env.observation_space.low\n",
    "        range_high = env.observation_space.high\n",
    "        \n",
    "        feat_ranges = list(zip(range_low, range_high))\n",
    "        \n",
    "        return feat_ranges\n",
    "    \n",
    "    ##Collect the relevant feedback from the environment\n",
    "    \n",
    "    def collect(self, state, action, reward, next_state, next_action, done):\n",
    "        \n",
    "        self.states.append([state, action, reward, next_state, next_action, done])\n",
    "     \n",
    "    ##Prepare the dataset for experience replay\n",
    "    \n",
    "    def prepareDataset(self, states):\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for vals in states:\n",
    "            \n",
    "            state, a, reward, next_state, next_action, done = vals\n",
    "            \n",
    "            feat = self.feat(state)\n",
    "            x = np.zeros((feat.shape[0] * self.num_actions, 1))\n",
    "            x[a * feat.shape[0]: (a + 1) * feat.shape[0]] = feat\n",
    "            s = x.shape[0]\n",
    "            x = np.append(x, [1]).reshape(s + 1, 1)\n",
    "            \n",
    "            X.append(x)\n",
    "            \n",
    "            if not done:  \n",
    "                target = reward + self.gamma * self.q(next_state, next_action)\n",
    "            else:\n",
    "                target = reward\n",
    "            \n",
    "            y.append(target)\n",
    "            \n",
    "        return np.squeeze(np.array(X)).T, np.expand_dims(np.array(y), axis = 1).T\n",
    "            \n",
    "    \n",
    "    ##Define a MSE loss\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        \n",
    "        y_pred = tf.matmul(self.W, X)\n",
    "        return tf.reduce_mean(tf.square(y - y_pred))\n",
    "    \n",
    "        \n",
    "    \n",
    "    ##Define the Q-function \n",
    "       \n",
    "    def q(self, state, a):\n",
    "        \n",
    "        feat = self.feat(state)\n",
    "        x = np.zeros((feat.shape[0] * self.num_actions, 1))\n",
    "        x[a * feat.shape[0]: (a + 1) * feat.shape[0]] = feat\n",
    "        \n",
    "        s = x.shape[0]\n",
    "        x = np.append(x, [1]).reshape(s + 1, 1)\n",
    "        \n",
    "        return tf.matmul(self.W, tf.convert_to_tensor(x, dtype = tf.float32))\n",
    "    \n",
    "    #Choose an action in a epsilong-greedy way\n",
    "    \n",
    "    def chooseAction(self, state):\n",
    "        \n",
    "        if (np.random.random() > self.epsilon):\n",
    "            vals = [self.q(state, a) for a in range(self.num_actions)]\n",
    "            return np.argmax(vals)\n",
    "        else:\n",
    "            return int(np.random.random() * self.num_actions)\n",
    "        \n",
    "    ##SARSA algorithm\n",
    "    \n",
    "    def updateReplay(self):\n",
    "        \n",
    "        if len(self.states) > BATCH_NUM:\n",
    "            states = random.choices(self.states, k = BATCH_NUM)\n",
    "        else:\n",
    "            states = self.states\n",
    "        \n",
    "        X, y = self.prepareDataset(states)\n",
    "        \n",
    "        ##Define a cost function for the optimizer\n",
    "        cost = lambda: self.loss(tf.convert_to_tensor(X, dtype = tf.float32), tf.convert_to_tensor(y, dtype = tf.float32))\n",
    "        \n",
    "        ##Perform a step of batch gradient descent\n",
    "        self.optimizer.minimize(cost, self.W)\n",
    "     \n",
    "    ##Get the coding\n",
    "    def feat(self, state):\n",
    "        coding = self.tilecode.tile(state)\n",
    "        return coding\n",
    "    \n",
    "    ##Flush the state memory of the last 200 states\n",
    "    def flush(self):\n",
    "        \n",
    "        self.states = self.states[200:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create agent\n",
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_STEP = 4\n",
    "NUM_EPISODES = 2000\n",
    "NUM_STEPS = 1000\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "\n",
    "    state = env.reset()\n",
    "    action = agent.chooseAction(state)\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(1, NUM_STEPS + 1):\n",
    "        env.render()\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_action = agent.chooseAction(next_state)\n",
    "        agent.collect(state, action, reward, next_state, next_action, done)\n",
    "        total_reward += reward\n",
    "        \n",
    "        ##Update the weights every UPDATE_STEP steps\n",
    "        if step%UPDATE_STEP == 0:\n",
    "            agent.updateReplay()\n",
    "        \n",
    "        if done:\n",
    "            ##Return some infos about the performance and append reward for later analysis\n",
    "            rewards.append(total_reward)\n",
    "            print(\"Episode \", episode,\": \", total_reward)\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "    \n",
    "    ##Flush the memory\n",
    "    if episode%20 == 0:\n",
    "        agent.flush()\n",
    "               \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
